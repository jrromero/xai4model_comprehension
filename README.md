# Can Explainable Artificial Intelligence Support Software Modelers in Model Comprehension?
_Supplementary material_ (March 15, 2024)

## Authors
- Francisco Javier Alcaide (a)
- José Raúl Romero (a - corresponding author)
- Aurora Ramírez (b)

(a) Dept. Computer Science and Numerical Analysis, University of Córdoba, 14071, Córdoba, Spain

(b) Dept. Computer Science and Programming Languages, University of Málaga, 29071, Málaga, Spain

## Abstract

As software systems become increasingly complex, the application of artificial intelligence (AI) in software engineering is gaining relevance. However, a critical gap exists in the understanding and interpretation of AI-driven decision-making processes, especially in areas intrinsically linked to human expertise, such as software modeling. This paper proposes an exploratory study on the feasibility, efficacy, and relevance of eXplainable Artificial Intelligence (XAI) techniques within this context. The application of machine learning (ML) to software models is relatively recent, so efforts such as the ModelSet dataset are crucial for deriving effective training data for ML models. In addition, predictive software modeling tasks present some particularities, such as the need for multi-class and multi-label approaches that are not as commonly investigated from a XAI perspective. In fact, the adoption of XAI in software modeling has been barely explored, and possibly requires adapted methodologies. Our approach encompasses an in-depth examination of explanations generated by XAI techniques to determine their effectiveness in clarifying ML model predictions. This could help software modelers understand, for example, why a model is classified in a particular domain. Additionally, this study conducts a survey among software modelers to capture how XAI-provided explanations support their decision-making and conclusions, with the aim of identifying current gaps and limitations. We argue that XAI can improve the transparency and trustworthiness of the decision-making process for software modelers, thereby fostering a deeper understanding of intricate modeling tasks.

## Supplementary material

### Datasets

We use the Modelset dataset provided by J.A. López Hernández, J.L. Cánovas-Izquierdo, and J. Sánchez Cuadrado in their paper: ["ModelSet: a dataset for machine learning in model-driven engineering"](https://doi.org/10.1007/s10270-021-00929-3). The dataset can be downloaded from [GitHub](https://github.com/modelset/modelset-dataset).

### Research questions

Our research aim to answer the following research questions:

- RQ1: What are the relevant characteristics of the classification models that explain their behavior in model comprehension tasks?
- RQ2: Do local explainable methods agree on relevant characteristics that explain specific predictions about software models?
- RQ3: How intuitive, complete and useful software modelers find the explanations generated by feature attribution methods?

### Content

This repository includes the following directories:

- [code](https://github.com/jrromero/xai4model_comprehension/tree/main/code): Notebooks and scripts required to replicate our experiments, divided by RQ.
- [data](https://github.com/jrromero/xai4model_comprehension/tree/main/data): Intermediate CSV files after preprocessing, divided by RQ.
- [results](https://github.com/jrromero/xai4model_comprehension/tree/main/results): Detailed results of classification performance and global explanations (RQ1), and local explanations (RQ2).
- [survey](https://github.com/jrromero/xai4model_comprehension/tree/main/survey): For RQ3, we provide the complete survey, aggregated responses and plots. 
